{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensuring Feature Consistency Between Training & InferencePipelines:\n",
    "\n",
    "**Task 1**: Consistent Feature Preparation\n",
    "- Step 1: Write a function for data preprocessing and imputation shared by both training and inference pipelines.\n",
    "- Step 2: Demonstrate consistent application on both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before preprocessing:\n",
      "Train:\n",
      "      A     B      C\n",
      "0  1.0  10.0    cat\n",
      "1  2.0   NaN    dog\n",
      "2  NaN  30.0    dog\n",
      "3  4.0  40.0    cat\n",
      "4  5.0  50.0  mouse\n",
      "Test:\n",
      "      A     B      C\n",
      "0  NaN  15.0    dog\n",
      "1  3.0   NaN    cat\n",
      "2  4.0  45.0  mouse\n",
      "\n",
      "After preprocessing:\n",
      "Train:\n",
      "      A     B      C\n",
      "0  1.0  10.0    cat\n",
      "1  2.0  35.0    dog\n",
      "2  3.0  30.0    dog\n",
      "3  4.0  40.0    cat\n",
      "4  5.0  50.0  mouse\n",
      "Test:\n",
      "      A     B      C\n",
      "0  3.0  15.0    dog\n",
      "1  3.0  35.0    cat\n",
      "2  4.0  45.0  mouse\n"
     ]
    }
   ],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def preprocess_data(df, imputer=None, fit_imputer=True):\n",
    "    \"\"\"\n",
    "    Preprocesses the input DataFrame by imputing missing values.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pd.DataFrame, input data to preprocess\n",
    "    - imputer: sklearn.imputer.SimpleImputer or None, the imputer instance to use\n",
    "    - fit_imputer: bool, whether to fit the imputer or just transform\n",
    "    \n",
    "    Returns:\n",
    "    - df_processed: pd.DataFrame, preprocessed data with imputed values\n",
    "    - imputer: fitted imputer instance (if fit_imputer=True), else same as input\n",
    "    \"\"\"\n",
    "    # For example, let's impute numeric columns with median\n",
    "    numeric_cols = df.select_dtypes(include='number').columns\n",
    "    \n",
    "    if imputer is None:\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "    \n",
    "    if fit_imputer:\n",
    "        df[numeric_cols] = imputer.fit_transform(df[numeric_cols])\n",
    "    else:\n",
    "        df[numeric_cols] = imputer.transform(df[numeric_cols])\n",
    "    \n",
    "    return df, imputer\n",
    "\n",
    "# Sample data with missing values\n",
    "train_df = pd.DataFrame({\n",
    "    'A': [1, 2, None, 4, 5],\n",
    "    'B': [10, None, 30, 40, 50],\n",
    "    'C': ['cat', 'dog', 'dog', 'cat', 'mouse']\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'A': [None, 3, 4],\n",
    "    'B': [15, None, 45],\n",
    "    'C': ['dog', 'cat', 'mouse']\n",
    "})\n",
    "\n",
    "print(\"Before preprocessing:\")\n",
    "print(\"Train:\\n\", train_df)\n",
    "print(\"Test:\\n\", test_df)\n",
    "\n",
    "# Preprocess train (fit imputer)\n",
    "train_processed, imputer = preprocess_data(train_df.copy(), fit_imputer=True)\n",
    "\n",
    "# Preprocess test (use same fitted imputer)\n",
    "test_processed, _ = preprocess_data(test_df.copy(), imputer=imputer, fit_imputer=False)\n",
    "\n",
    "print(\"\\nAfter preprocessing:\")\n",
    "print(\"Train:\\n\", train_processed)\n",
    "print(\"Test:\\n\", test_processed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Pipeline Integration\n",
    "- Step 1: Use sklearn pipelines to encapsulate the preprocessing steps.\n",
    "- Step 2: Configure identical pipelines for both training and building inference models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Sample dataset: Titanic (contains numeric & categorical data + missing values)\n",
    "titanic = fetch_openml('titanic', version=1, as_frame=True)\n",
    "df = titanic.frame\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop(columns='survived')\n",
    "y = df['survived'].astype(int)\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['category', 'object']).columns.tolist()\n",
    "\n",
    "# Numeric pipeline: impute with median, then scale\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline: impute with most frequent, then one-hot encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine pipelines\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on new data: [0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Full pipeline: preprocessing + classifier\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Fit on training data\n",
    "model_pipeline.fit(X, y)\n",
    "\n",
    "# Example: Predict on new data (inference)\n",
    "X_new = X.sample(5, random_state=42)  # simulate new data\n",
    "predictions = model_pipeline.predict(X_new)\n",
    "\n",
    "print(\"Predictions on new data:\", predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Saving and Loading Preprocessing Models\n",
    "- Step 1: Save the transformation model after fitting it to the training data.\n",
    "- Step 2: Load and apply the saved model during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline saved to titanic_model_pipeline.joblib\n",
      "Pipeline loaded.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "columns are missing: {'name', 'cabin', 'boat', 'body', 'ticket', 'home.dest'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 36\u001b[0m\n\u001b[1;32m     18\u001b[0m new_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpclass\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m22\u001b[39m, \u001b[38;5;241m38\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malone\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m]\n\u001b[1;32m     33\u001b[0m })\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Predict with loaded pipeline\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mloaded_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions on new data:\u001b[39m\u001b[38;5;124m\"\u001b[39m, preds)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/pipeline.py:787\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[0;34m(self, X, **params)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 787\u001b[0m         Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    790\u001b[0m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:1090\u001b[0m, in \u001b[0;36mColumnTransformer.transform\u001b[0;34m(self, X, **params)\u001b[0m\n\u001b[1;32m   1088\u001b[0m     diff \u001b[38;5;241m=\u001b[39m all_names \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(column_names)\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m diff:\n\u001b[0;32m-> 1090\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns are missing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiff\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;66;03m# ndarray was used for fitting or transforming, thus we only\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;66;03m# check that n_features_in_ is consistent\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     _check_n_features(\u001b[38;5;28mself\u001b[39m, X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: columns are missing: {'name', 'cabin', 'boat', 'body', 'ticket', 'home.dest'}"
     ]
    }
   ],
   "source": [
    "# write your code from here\n",
    "import joblib\n",
    "\n",
    "# Assume model_pipeline from previous step is already fitted\n",
    "filename = 'titanic_model_pipeline.joblib'\n",
    "\n",
    "# Save the entire pipeline (preprocessing + model)\n",
    "joblib.dump(model_pipeline, filename)\n",
    "print(f\"Pipeline saved to {filename}\")\n",
    "\n",
    "# Load the pipeline\n",
    "loaded_pipeline = joblib.load(filename)\n",
    "print(\"Pipeline loaded.\")\n",
    "\n",
    "# Example new data (simulate inference data)\n",
    "import pandas as pd\n",
    "\n",
    "new_data = pd.DataFrame({\n",
    "    'pclass': [3, 1],\n",
    "    'age': [22, 38],\n",
    "    'sibsp': [1, 1],\n",
    "    'parch': [0, 0],\n",
    "    'fare': [7.25, 71.2833],\n",
    "    'sex': ['male', 'female'],\n",
    "    'embarked': ['S', 'C'],\n",
    "    'deck': [None, 'C'],\n",
    "    'embark_town': ['Southampton', 'Cherbourg'],\n",
    "    'alive': ['no', 'yes'],\n",
    "    'class': ['Third', 'First'],\n",
    "    'who': ['man', 'woman'],\n",
    "    'adult_male': [True, False],\n",
    "    'alone': [False, False]\n",
    "})\n",
    "\n",
    "# Predict with loaded pipeline\n",
    "preds = loaded_pipeline.predict(new_data)\n",
    "print(\"Predictions on new data:\", preds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
