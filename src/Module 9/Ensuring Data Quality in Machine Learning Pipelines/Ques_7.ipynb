{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering Best Practices: Handling Text Data\n",
    "**Question**: Load a dataset with text data (e.g., SMS Spam Collection), perform text\n",
    "preprocessing, and extract numerical features using TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/vscode/.local/lib/python3.10/site-packages (2.1.4)\n",
      "Requirement already satisfied: scikit-learn in /home/vscode/.local/lib/python3.10/site-packages (1.6.1)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: tzdata>=2022.1 in /home/vscode/.local/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /home/vscode/.local/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/vscode/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/vscode/.local/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/vscode/.local/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/vscode/.local/lib/python3.10/site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/vscode/.local/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/vscode/.local/lib/python3.10/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: click in /home/vscode/.local/lib/python3.10/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/vscode/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.9.1 regex-2024.11.6\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# write your code from here\n",
    "!pip install pandas scikit-learn nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/vscode/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔤 Sample TF-IDF Features:\n",
      "   0800  08000839402  08000930705   10       100  1000  10p  10pmin  11mth  \\\n",
      "0   0.0          0.0          0.0  0.0  0.366331   0.0  0.0     0.0    0.0   \n",
      "1   0.0          0.0          0.0  0.0  0.000000   0.0  0.0     0.0    0.0   \n",
      "2   0.0          0.0          0.0  0.0  0.000000   0.0  0.0     0.0    0.0   \n",
      "3   0.0          0.0          0.0  0.0  0.000000   0.0  0.0     0.0    0.0   \n",
      "4   0.0          0.0          0.0  0.0  0.000000   0.0  0.0     0.0    0.0   \n",
      "\n",
      "    12  ...  year  yep  yesterday  yet   yo  youll  your  youv   yr  yup  \n",
      "0  0.0  ...   0.0  0.0        0.0  0.0  0.0    0.0   0.0   0.0  0.0  0.0  \n",
      "1  0.0  ...   0.0  0.0        0.0  0.0  0.0    0.0   0.0   0.0  0.0  0.0  \n",
      "2  0.0  ...   0.0  0.0        0.0  0.0  0.0    0.0   0.0   0.0  0.0  0.0  \n",
      "3  0.0  ...   0.0  0.0        0.0  0.0  0.0    0.0   0.0   0.0  0.0  0.0  \n",
      "4  0.0  ...   0.0  0.0        0.0  0.0  0.0    0.0   0.0   0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 1000 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Step 1: Load SMS Spam dataset\n",
    "url = \"https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv\"\n",
    "df = pd.read_csv(url, sep='\\t', header=None, names=[\"label\", \"message\"])\n",
    "\n",
    "# Step 2: Encode labels (ham=0, spam=1)\n",
    "df[\"label\"] = LabelEncoder().fit_transform(df[\"label\"])\n",
    "\n",
    "# Step 3: Text preprocessing\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # lowercase\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # remove punctuation\n",
    "    tokens = text.split()\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]  # remove stopwords + stem\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"cleaned\"] = df[\"message\"].apply(preprocess_text)\n",
    "\n",
    "# Step 4: Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"cleaned\"], df[\"label\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 5: TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Step 6: Show example features\n",
    "print(\"\\n🔤 Sample TF-IDF Features:\")\n",
    "print(pd.DataFrame(X_train_tfidf.toarray(), columns=vectorizer.get_feature_names_out()).head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
