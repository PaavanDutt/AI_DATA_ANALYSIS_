{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Deduplication using Clustering\n",
    "**Objective**: Learn and implement data deduplication techniques.\n",
    "\n",
    "**Task**: DBSCAN for Data Deduplication\n",
    "\n",
    "**Steps**:\n",
    "1. Data Set: Download a dataset containing duplicate entries for event registrations.\n",
    "2. DBSCAN Clustering: Apply the DBSCAN algorithm to cluster similar registrations.\n",
    "3. Identify Duplicates: Detect duplicates based on density of the clusters.\n",
    "4. Refinement: Validate clusters and remove any erroneous duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Event Registrations Dataset:\n",
      "    RegistrationID RegistrantName           RegistrantEmail  \\\n",
      "0                1   Alex Johnson        alex.j@example.com   \n",
      "1                2   Sarah Miller       sarah.m@example.com   \n",
      "2                3      David Lee       david.l@example.com   \n",
      "3                4     Emily Chen       emily.c@example.com   \n",
      "4                5  Michael Brown     michael.b@example.com   \n",
      "5                6    Alex Jonson  alex.johnson@example.com   \n",
      "6                7    Sara Miller      s.miller@example.com   \n",
      "7                8       Dave Lee         d.lee@example.com   \n",
      "8                9      Emly Chen        e.chen@example.com   \n",
      "9               10     Mike Brown       m.brown@example.com   \n",
      "10              11   Alex Johnson        alex.j@example.com   \n",
      "11              12    Sarah Miler  sarah.miller@example.com   \n",
      "12              13      David Lee       david.l@example.com   \n",
      "\n",
      "                 EventTitle  \n",
      "0        Annual Tech Summit  \n",
      "1        Marketing Workshop  \n",
      "2   Data Science Conference  \n",
      "3        AI Innovations Day  \n",
      "4      Product Launch Event  \n",
      "5               Tech Summit  \n",
      "6        Marketing Workshop  \n",
      "7        Data Science Conf.  \n",
      "8         AI Innovation Day  \n",
      "9            Product Launch  \n",
      "10       Annual Tech Summit  \n",
      "11        Marketing Workshp  \n",
      "12  Data Science Conference  \n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Negative values in data passed to X.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 74\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# metric='precomputed' tells DBSCAN to use the distance_matrix directly.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m dbscan \u001b[38;5;241m=\u001b[39m DBSCAN(eps\u001b[38;5;241m=\u001b[39meps, min_samples\u001b[38;5;241m=\u001b[39mmin_samples, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 74\u001b[0m clusters \u001b[38;5;241m=\u001b[39m \u001b[43mdbscan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistance_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Add cluster IDs to the DataFrame\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# -1 indicates noise points (outliers) that do not belong to any cluster.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClusterID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m clusters\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/cluster/_dbscan.py:470\u001b[0m, in \u001b[0;36mDBSCAN.fit_predict\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    446\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute clusters from a data or distance matrix and predict labels.\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \n\u001b[1;32m    448\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;124;03m        Cluster labels. Noisy samples are given the label -1.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 470\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels_\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/cluster/_dbscan.py:416\u001b[0m, in \u001b[0;36mDBSCAN.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    405\u001b[0m         X\u001b[38;5;241m.\u001b[39msetdiag(X\u001b[38;5;241m.\u001b[39mdiagonal())\n\u001b[1;32m    407\u001b[0m neighbors_model \u001b[38;5;241m=\u001b[39m NearestNeighbors(\n\u001b[1;32m    408\u001b[0m     radius\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps,\n\u001b[1;32m    409\u001b[0m     algorithm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    414\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[1;32m    415\u001b[0m )\n\u001b[0;32m--> 416\u001b[0m \u001b[43mneighbors_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# This has worst case O(n^2) memory complexity\u001b[39;00m\n\u001b[1;32m    418\u001b[0m neighborhoods \u001b[38;5;241m=\u001b[39m neighbors_model\u001b[38;5;241m.\u001b[39mradius_neighbors(X, return_distance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/neighbors/_unsupervised.py:179\u001b[0m, in \u001b[0;36mNearestNeighbors.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# NearestNeighbors.metric is not validated yet\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    161\u001b[0m )\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    163\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the nearest neighbors estimator from the training dataset.\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m        The fitted nearest neighbors estimator.\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/neighbors/_base.py:583\u001b[0m, in \u001b[0;36mNeighborsBase._fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 583\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43m_check_precomputed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;66;03m# Precomputed matrix X must be squared\u001b[39;00m\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/neighbors/_base.py:173\u001b[0m, in \u001b[0;36m_check_precomputed\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check precomputed distance matrix.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03mIf the precomputed distance matrix is sparse, it checks that the non-zero\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m    case only non-zero elements may be considered neighbors.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(X):\n\u001b[0;32m--> 173\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_non_negative\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:1149\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m estimator_name:\n\u001b[1;32m   1148\u001b[0m         whom \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1149\u001b[0m     \u001b[43mcheck_non_negative\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhom\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_writeable:\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;66;03m# By default, array.copy() creates a C-ordered copy. We set order=K to\u001b[39;00m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;66;03m# preserve the order of the array.\u001b[39;00m\n\u001b[1;32m   1154\u001b[0m     copy_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m\"\u001b[39m} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(array) \u001b[38;5;28;01melse\u001b[39;00m {}\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:1827\u001b[0m, in \u001b[0;36mcheck_non_negative\u001b[0;34m(X, whom)\u001b[0m\n\u001b[1;32m   1824\u001b[0m     X_min \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mmin(X)\n\u001b[1;32m   1826\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_min \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1827\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNegative values in data passed to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwhom\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Negative values in data passed to X."
     ]
    }
   ],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Data Set - Create a sample dataset for event registrations with duplicate entries.\n",
    "# We'll simulate variations in names, emails, and event titles.\n",
    "data = {\n",
    "    'RegistrationID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13],\n",
    "    'RegistrantName': [\n",
    "        'Alex Johnson', 'Sarah Miller', 'David Lee', 'Emily Chen', 'Michael Brown',\n",
    "        'Alex Jonson', 'Sara Miller', 'Dave Lee', 'Emly Chen', 'Mike Brown',\n",
    "        'Alex Johnson', # Exact duplicate of ID 1\n",
    "        'Sarah Miler', # Typo in 'Miller'\n",
    "        'David Lee' # Exact duplicate of ID 3\n",
    "    ],\n",
    "    'RegistrantEmail': [\n",
    "        'alex.j@example.com', 'sarah.m@example.com', 'david.l@example.com',\n",
    "        'emily.c@example.com', 'michael.b@example.com',\n",
    "        'alex.johnson@example.com', 's.miller@example.com', 'd.lee@example.com',\n",
    "        'e.chen@example.com', 'm.brown@example.com',\n",
    "        'alex.j@example.com', # Exact duplicate of ID 1's email\n",
    "        'sarah.miller@example.com', # Similar to ID 2\n",
    "        'david.l@example.com' # Exact duplicate of ID 3's email\n",
    "    ],\n",
    "    'EventTitle': [\n",
    "        'Annual Tech Summit', 'Marketing Workshop', 'Data Science Conference',\n",
    "        'AI Innovations Day', 'Product Launch Event',\n",
    "        'Tech Summit', 'Marketing Workshop', 'Data Science Conf.',\n",
    "        'AI Innovation Day', 'Product Launch',\n",
    "        'Annual Tech Summit',\n",
    "        'Marketing Workshp',\n",
    "        'Data Science Conference'\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original Event Registrations Dataset:\")\n",
    "print(df)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# For deduplication, combine relevant text fields into a single string for similarity calculation.\n",
    "# Lowercasing and stripping whitespace helps normalize the data.\n",
    "df['Combined_Info'] = df['RegistrantName'].str.lower().str.strip() + \" \" + \\\n",
    "                      df['RegistrantEmail'].str.lower().str.strip() + \" \" + \\\n",
    "                      df['EventTitle'].str.lower().str.strip()\n",
    "\n",
    "# Step 2: DBSCAN Clustering - Apply the DBSCAN algorithm\n",
    "# 2.1 Feature Extraction: Convert text data into numerical features using TF-IDF\n",
    "vectorizer = TfidfVectorizer().fit_transform(df['Combined_Info'])\n",
    "\n",
    "# 2.2 Calculate Similarity Matrix: Compute cosine similarity between records\n",
    "# DBSCAN works with distances, so we'll convert similarity to distance.\n",
    "similarity_matrix = cosine_similarity(vectorizer)\n",
    "distance_matrix = 1 - similarity_matrix\n",
    "\n",
    "# DBSCAN requires a dense distance matrix or precomputed distances.\n",
    "# We'll use the precomputed distance matrix.\n",
    "\n",
    "# 2.3 Apply DBSCAN\n",
    "# eps: The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
    "# min_samples: The number of samples (or total weight) in a neighborhood for a point to be considered as a core point.\n",
    "# Choosing eps and min_samples is crucial and often requires domain knowledge or experimentation.\n",
    "# For text similarity (where 1-similarity is distance), a small eps value means high similarity.\n",
    "# Let's start with a small eps, e.g., 0.3 (meaning similarity >= 0.7) and min_samples=2.\n",
    "# A point is a core point if it has at least min_samples (including itself) within eps distance.\n",
    "# Adjust these parameters based on your data and desired cluster density.\n",
    "eps = 0.3 # Max distance for points to be considered neighbors (corresponds to min similarity of 0.7)\n",
    "min_samples = 2 # Minimum number of samples in a neighborhood for a point to be a core point\n",
    "\n",
    "# metric='precomputed' tells DBSCAN to use the distance_matrix directly.\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed')\n",
    "clusters = dbscan.fit_predict(distance_matrix)\n",
    "\n",
    "# Add cluster IDs to the DataFrame\n",
    "# -1 indicates noise points (outliers) that do not belong to any cluster.\n",
    "df['ClusterID'] = clusters\n",
    "\n",
    "print(\"Dataset with DBSCAN Cluster IDs:\")\n",
    "print(df[['RegistrationID', 'RegistrantName', 'RegistrantEmail', 'EventTitle', 'ClusterID']])\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Step 3: Identify Duplicates - Detect duplicates based on density of the clusters.\n",
    "# Duplicates are typically found in clusters with more than one member (ClusterID != -1).\n",
    "# Noise points (ClusterID = -1) are considered unique or unclassifiable.\n",
    "duplicate_clusters_df = df[df['ClusterID'] != -1].groupby('ClusterID').filter(lambda x: len(x) > 1)\n",
    "\n",
    "print(f\"Identified Duplicate Records (Clusters with more than 1 member, excluding noise):\")\n",
    "print(duplicate_clusters_df[['RegistrationID', 'RegistrantName', 'RegistrantEmail', 'EventTitle', 'ClusterID']])\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Step 4: Refinement - Validate clusters and remove any erroneous duplicates.\n",
    "# For each identified cluster, we'll keep one representative record.\n",
    "# A common strategy is to keep the record with the lowest RegistrationID.\n",
    "clean_df_list = []\n",
    "for cluster_id in df['ClusterID'].unique():\n",
    "    cluster_records = df[df['ClusterID'] == cluster_id]\n",
    "    if cluster_id == -1: # Keep all noise points as they are considered unique\n",
    "        clean_df_list.append(cluster_records)\n",
    "    else: # For actual clusters, keep only one record\n",
    "        # Keep the record with the minimum RegistrationID as the representative\n",
    "        representative_record = cluster_records.loc[cluster_records['RegistrationID'].idxmin()]\n",
    "        clean_df_list.append(pd.DataFrame([representative_record]))\n",
    "\n",
    "clean_df = pd.concat(clean_df_list).reset_index(drop=True)\n",
    "\n",
    "# Drop the 'Combined_Info' column as it was for internal processing\n",
    "clean_df = clean_df.drop(columns=['Combined_Info'])\n",
    "\n",
    "print(\"Cleaned Dataset (Duplicates Removed by DBSCAN):\")\n",
    "print(clean_df[['RegistrationID', 'RegistrantName', 'RegistrantEmail', 'EventTitle']])\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Verification: Check if any of the original duplicates are still present in the cleaned data\n",
    "# Define some expected duplicate pairs based on the sample data\n",
    "expected_duplicate_pairs = [\n",
    "    ('Alex Johnson', 'Alex Jonson'),\n",
    "    ('Alex Johnson', 'Alex Johnson'), # Exact duplicate\n",
    "    ('Sarah Miller', 'Sara Miller'),\n",
    "    ('Sarah Miller', 'Sarah Miler'),\n",
    "    ('David Lee', 'Dave Lee'),\n",
    "    ('David Lee', 'David Lee') # Exact duplicate\n",
    "]\n",
    "\n",
    "print(\"Verification of Cleaned Data:\")\n",
    "for name1, name2 in expected_duplicate_pairs:\n",
    "    # Check if both names from a pair are still in the cleaned data (indicating a potential missed duplicate)\n",
    "    # This check is simplified; a more robust check would involve comparing full records.\n",
    "    if name1 in clean_df['RegistrantName'].values and name2 in clean_df['RegistrantName'].values and name1 != name2:\n",
    "        # Check if they are in different clusters or if both were kept as unique\n",
    "        # This is a rough check, as DBSCAN might classify one as noise and the other as unique.\n",
    "        print(f\"Warning: '{name1}' and '{name2}' (or similar) might still be present. DBSCAN parameters (eps, min_samples) might need adjustment.\")\n",
    "    elif name1 in clean_df['RegistrantName'].values or name2 in clean_df['RegistrantName'].values:\n",
    "        print(f\"One of '{name1}' or '{name2}' is present, which is expected after deduplication.\")\n",
    "    else:\n",
    "        print(f\"Neither '{name1}' nor '{name2}' found, indicating successful deduplication for this pair.\")\n",
    "\n",
    "# To see the actual records that were removed:\n",
    "# Records removed are those that were part of a cluster but not selected as the representative,\n",
    "# or those that were noise but should have been part of a cluster (if eps/min_samples were different).\n",
    "removed_records = df[~df['RegistrationID'].isin(clean_df['RegistrationID'])]\n",
    "print(\"\\nRecords that were removed as duplicates:\")\n",
    "print(removed_records[['RegistrationID', 'RegistrantName', 'RegistrantEmail', 'ClusterID']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
