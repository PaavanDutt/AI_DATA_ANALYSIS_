{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NLP for Text Data Quality\n",
    "**Objective**: Enhance text data quality using NLP techniques.\n",
    "\n",
    "**Task**: Removing Stopwords\n",
    "\n",
    "**Steps**:\n",
    "1. Data Set: Use a dataset of text product descriptions.\n",
    "2. Stopword Removal: Utilize an NLP library (e.g., NLTK) to remove stopwords from the\n",
    "descriptions.\n",
    "3. Assess Impact: Examine the effectiveness by analyzing word frequency before and after\n",
    "removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset with Noisy Text:\n",
      "   ReviewID                                     CustomerReview  \\\n",
      "0         1                  Great product! Loved it. #awesome   \n",
      "1         2  This is a @good_service, but delivery was slow. üöö   \n",
      "2         3   The quality is amazing!!! üíØ (highly recommended)   \n",
      "3         4             Product was ok. Had some issues. $$$$$   \n",
      "4         5  Terrible experience. Customer support is non-e...   \n",
      "5         6  A bit pricey, but worth it. Check out: http://...   \n",
      "\n",
      "                                           NoisyText  \n",
      "0  Th1s 1s s0me r@nd0m t3xt w1th numb3rs and symb...  \n",
      "1  Another review with [junk] characters and <htm...  \n",
      "2  Good product, but the instructions were confus...  \n",
      "3    I received a broken item. Refund requested. üò†üò°ü§¨  \n",
      "4                     Excellent! Very satisfied. üòäüëçüëç  \n",
      "5                 Just some text without much noise.  \n",
      "\n",
      "==================================================\n",
      "\n",
      "Dataset After Cleaning:\n",
      "   ReviewID                                     CustomerReview  \\\n",
      "0         1                  Great product! Loved it. #awesome   \n",
      "1         2  This is a @good_service, but delivery was slow. üöö   \n",
      "2         3   The quality is amazing!!! üíØ (highly recommended)   \n",
      "3         4             Product was ok. Had some issues. $$$$$   \n",
      "4         5  Terrible experience. Customer support is non-e...   \n",
      "5         6  A bit pricey, but worth it. Check out: http://...   \n",
      "\n",
      "                              Cleaned_CustomerReview  \\\n",
      "0                             Great product Loved it   \n",
      "1                    This is a but delivery was slow   \n",
      "2          The quality is amazing highly recommended   \n",
      "3                     Product was ok Had some issues   \n",
      "4  Terrible experience Customer support is nonexi...   \n",
      "5                A bit pricey but worth it Check out   \n",
      "\n",
      "                                           NoisyText  \\\n",
      "0  Th1s 1s s0me r@nd0m t3xt w1th numb3rs and symb...   \n",
      "1  Another review with [junk] characters and <htm...   \n",
      "2  Good product, but the instructions were confus...   \n",
      "3    I received a broken item. Refund requested. üò†üò°ü§¨   \n",
      "4                     Excellent! Very satisfied. üòäüëçüëç   \n",
      "5                 Just some text without much noise.   \n",
      "\n",
      "                                   Cleaned_NoisyText  \n",
      "0       Th1s 1s s0me r t3xt w1th numb3rs and symb0ls  \n",
      "1  Another review with junk characters and html tags  \n",
      "2   Good product but the instructions were confusing  \n",
      "3          I received a broken item Refund requested  \n",
      "4                           Excellent Very satisfied  \n",
      "5                  Just some text without much noise  \n",
      "\n",
      "==================================================\n",
      "\n",
      "Comparison of Original vs. Cleaned Text (Noise Removal):\n",
      "Review ID: 1\n",
      "  Original Customer Review: 'Great product! Loved it. #awesome'\n",
      "  Cleaned Customer Review:  'Great product Loved it'\n",
      "  Original Noisy Text:    'Th1s 1s s0me r@nd0m t3xt w1th numb3rs and symb0ls! ^&*()'\n",
      "  Cleaned Noisy Text:     'Th1s 1s s0me r t3xt w1th numb3rs and symb0ls'\n",
      "------------------------------\n",
      "Review ID: 2\n",
      "  Original Customer Review: 'This is a @good_service, but delivery was slow. üöö'\n",
      "  Cleaned Customer Review:  'This is a but delivery was slow'\n",
      "  Original Noisy Text:    'Another review with [junk] characters and <html tags>.'\n",
      "  Cleaned Noisy Text:     'Another review with junk characters and html tags'\n",
      "------------------------------\n",
      "Review ID: 3\n",
      "  Original Customer Review: 'The quality is amazing!!! üíØ (highly recommended)'\n",
      "  Cleaned Customer Review:  'The quality is amazing highly recommended'\n",
      "  Original Noisy Text:    'Good product, but the instructions were confusing. üöÄ‚ú®'\n",
      "  Cleaned Noisy Text:     'Good product but the instructions were confusing'\n",
      "------------------------------\n",
      "Review ID: 4\n",
      "  Original Customer Review: 'Product was ok. Had some issues. $$$$$'\n",
      "  Cleaned Customer Review:  'Product was ok Had some issues'\n",
      "  Original Noisy Text:    'I received a broken item. Refund requested. üò†üò°ü§¨'\n",
      "  Cleaned Noisy Text:     'I received a broken item Refund requested'\n",
      "------------------------------\n",
      "Review ID: 5\n",
      "  Original Customer Review: 'Terrible experience. Customer support is non-existent. üò°üò°üò°'\n",
      "  Cleaned Customer Review:  'Terrible experience Customer support is nonexistent'\n",
      "  Original Noisy Text:    'Excellent! Very satisfied. üòäüëçüëç'\n",
      "  Cleaned Noisy Text:     'Excellent Very satisfied'\n",
      "------------------------------\n",
      "Review ID: 6\n",
      "  Original Customer Review: 'A bit pricey, but worth it. Check out: http://example.com/product'\n",
      "  Cleaned Customer Review:  'A bit pricey but worth it Check out'\n",
      "  Original Noisy Text:    'Just some text without much noise.'\n",
      "  Cleaned Noisy Text:     'Just some text without much noise'\n",
      "------------------------------\n",
      "\n",
      "##################################################\n",
      "\n",
      "### Task: Removing Stopwords ###\n",
      "\n",
      "##################################################\n",
      "\n",
      "Original Product Descriptions Dataset:\n",
      "   ProductID                                        Description\n",
      "0          1  This is a very good product with excellent fea...\n",
      "1          2  The quick brown fox jumps over the lazy dog. I...\n",
      "2          3  An amazing gadget for everyday use. It has man...\n",
      "3          4  I am really happy with this purchase. It was d...\n",
      "4          5  A high-quality item that will last for many ye...\n",
      "\n",
      "==================================================\n",
      "\n",
      "Product Descriptions After Stopword Removal:\n",
      "   ProductID                                        Description  \\\n",
      "0          1  This is a very good product with excellent fea...   \n",
      "1          2  The quick brown fox jumps over the lazy dog. I...   \n",
      "2          3  An amazing gadget for everyday use. It has man...   \n",
      "3          4  I am really happy with this purchase. It was d...   \n",
      "4          5  A high-quality item that will last for many ye...   \n",
      "\n",
      "                    Cleaned_Description_No_Stopwords  \n",
      "0  good product excellent features long battery l...  \n",
      "1   quick brown fox jumps lazy dog. classic example.  \n",
      "2  amazing gadget everyday use. many useful funct...  \n",
      "3          really happy purchase. delivered quickly.  \n",
      "4         high-quality item last many years. buy it.  \n",
      "\n",
      "==================================================\n",
      "\n",
      "Word Frequencies Before Stopword Removal:\n",
      "  'a': 4\n",
      "  'it': 3\n",
      "  'this': 2\n",
      "  'is': 2\n",
      "  'with': 2\n",
      "  'the': 2\n",
      "  'for': 2\n",
      "  'many': 2\n",
      "  'very': 1\n",
      "  'good': 1\n",
      "  'product': 1\n",
      "  'excellent': 1\n",
      "  'features': 1\n",
      "  'and': 1\n",
      "  'long': 1\n",
      "  'battery': 1\n",
      "  'life.': 1\n",
      "  'quick': 1\n",
      "  'brown': 1\n",
      "  'fox': 1\n",
      "\n",
      "\n",
      "Word Frequencies After Stopword Removal:\n",
      "  'many': 2\n",
      "  'good': 1\n",
      "  'product': 1\n",
      "  'excellent': 1\n",
      "  'features': 1\n",
      "  'long': 1\n",
      "  'battery': 1\n",
      "  'life.': 1\n",
      "  'quick': 1\n",
      "  'brown': 1\n",
      "  'fox': 1\n",
      "  'jumps': 1\n",
      "  'lazy': 1\n",
      "  'dog.': 1\n",
      "  'classic': 1\n",
      "  'example.': 1\n",
      "  'amazing': 1\n",
      "  'gadget': 1\n",
      "  'everyday': 1\n",
      "  'use.': 1\n",
      "\n",
      "==================================================\n",
      "\n",
      "Observation:\n",
      "You can observe that common words like 'is', 'a', 'the', 'with', 'for', 'it', 'has', 'was', 'will', 'you', 'should' etc., which are typically stopwords, have significantly reduced or disappeared from the 'Word Frequencies After Stopword Removal' list, indicating the effectiveness of the process.\n"
     ]
    }
   ],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# Download NLTK stopwords if not already downloaded\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    print(\"NLTK 'stopwords' corpus downloaded.\")\n",
    "\n",
    "# Step 1: Data Set - Obtain a dataset with customer reviews containing noise.\n",
    "# We'll create a sample DataFrame with various types of noise.\n",
    "data = {\n",
    "    'ReviewID': [1, 2, 3, 4, 5, 6],\n",
    "    'CustomerReview': [\n",
    "        'Great product! Loved it. #awesome',\n",
    "        'This is a @good_service, but delivery was slow. üöö',\n",
    "        'The quality is amazing!!! üíØ (highly recommended)',\n",
    "        'Product was ok. Had some issues. $$$$$',\n",
    "        'Terrible experience. Customer support is non-existent. üò°üò°üò°',\n",
    "        'A bit pricey, but worth it. Check out: http://example.com/product'\n",
    "    ],\n",
    "    'NoisyText': [\n",
    "        'Th1s 1s s0me r@nd0m t3xt w1th numb3rs and symb0ls! ^&*()',\n",
    "        'Another review with [junk] characters and <html tags>.',\n",
    "        'Good product, but the instructions were confusing. üöÄ‚ú®',\n",
    "        'I received a broken item. Refund requested. üò†üò°ü§¨',\n",
    "        'Excellent! Very satisfied. üòäüëçüëç',\n",
    "        'Just some text without much noise.'\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original Dataset with Noisy Text:\")\n",
    "print(df[['ReviewID', 'CustomerReview', 'NoisyText']])\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Step 2: Clean Data - Use regex patterns to clean the noise from text data.\n",
    "# We'll define a function that applies multiple regex patterns for cleaning.\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans text data by removing various types of noise using regex.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input string to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned string.\n",
    "    \"\"\"\n",
    "    # Convert text to string to handle potential non-string types\n",
    "    text = str(text)\n",
    "\n",
    "    # 1. Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # 2. Remove mentions (@username)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # 3. Remove hashtags (#hashtag)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # 4. Remove emojis (basic range, more comprehensive regex might be needed for all emojis)\n",
    "    # This regex matches common emoji ranges.\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        \"\\U00002702-\\U000027B0\"\n",
    "        \"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    # 5. Remove punctuation and special characters (keep alphanumeric and spaces)\n",
    "    # This regex keeps letters, numbers, and spaces.\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    # 6. Remove extra whitespace (multiple spaces, leading/trailing spaces)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # 7. Remove numbers if desired (uncomment if numbers are considered noise)\n",
    "    # text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the 'CustomerReview' and 'NoisyText' columns\n",
    "df['Cleaned_CustomerReview'] = df['CustomerReview'].apply(clean_text)\n",
    "df['Cleaned_NoisyText'] = df['NoisyText'].apply(clean_text)\n",
    "\n",
    "# Step 3: Evaluate - Compare the text before and after cleaning for noise.\n",
    "print(\"Dataset After Cleaning:\")\n",
    "print(df[['ReviewID', 'CustomerReview', 'Cleaned_CustomerReview', 'NoisyText', 'Cleaned_NoisyText']])\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Comparison of Original vs. Cleaned Text (Noise Removal):\")\n",
    "for index, row in df.iterrows():\n",
    "    print(f\"Review ID: {row['ReviewID']}\")\n",
    "    print(f\"  Original Customer Review: '{row['CustomerReview']}'\")\n",
    "    print(f\"  Cleaned Customer Review:  '{row['Cleaned_CustomerReview']}'\")\n",
    "    print(f\"  Original Noisy Text:    '{row['NoisyText']}'\")\n",
    "    print(f\"  Cleaned Noisy Text:     '{row['Cleaned_NoisyText']}'\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "print(\"\\n\" + \"#\"*50 + \"\\n\")\n",
    "print(\"### Task: Removing Stopwords ###\")\n",
    "print(\"\\n\" + \"#\"*50 + \"\\n\")\n",
    "\n",
    "# Step 1: Data Set - Use a dataset of text product descriptions.\n",
    "product_data = {\n",
    "    'ProductID': [1, 2, 3, 4, 5],\n",
    "    'Description': [\n",
    "        'This is a very good product with excellent features and a long battery life.',\n",
    "        'The quick brown fox jumps over the lazy dog. It is a classic example.',\n",
    "        'An amazing gadget for everyday use. It has many useful functions.',\n",
    "        'I am really happy with this purchase. It was delivered quickly.',\n",
    "        'A high-quality item that will last for many years. You should buy it.'\n",
    "    ]\n",
    "}\n",
    "product_df = pd.DataFrame(product_data)\n",
    "print(\"Original Product Descriptions Dataset:\")\n",
    "print(product_df)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Get English stopwords from NLTK\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Define a function for stopword removal\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Removes stopwords from a given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input string.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with stopwords removed.\n",
    "    \"\"\"\n",
    "    # Ensure text is string and convert to lowercase for consistent matching\n",
    "    text = str(text).lower()\n",
    "    # Tokenize the text (split into words) and remove stopwords\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in english_stopwords]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply stopword removal to the 'Description' column\n",
    "product_df['Cleaned_Description_No_Stopwords'] = product_df['Description'].apply(remove_stopwords)\n",
    "\n",
    "print(\"Product Descriptions After Stopword Removal:\")\n",
    "print(product_df[['ProductID', 'Description', 'Cleaned_Description_No_Stopwords']])\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Step 3: Assess Impact - Examine the effectiveness by analyzing word frequency before and after removal.\n",
    "\n",
    "def get_word_frequencies(text_series):\n",
    "    \"\"\"\n",
    "    Calculates word frequencies for a given pandas Series of text.\n",
    "\n",
    "    Args:\n",
    "        text_series (pd.Series): A Series containing text strings.\n",
    "\n",
    "    Returns:\n",
    "        collections.Counter: A Counter object with word frequencies.\n",
    "    \"\"\"\n",
    "    all_words = []\n",
    "    for text in text_series:\n",
    "        # Convert to lowercase and split into words\n",
    "        all_words.extend(str(text).lower().split())\n",
    "    return Counter(all_words)\n",
    "\n",
    "print(\"Word Frequencies Before Stopword Removal:\")\n",
    "original_frequencies = get_word_frequencies(product_df['Description'])\n",
    "# Print top 20 most common words\n",
    "for word, count in original_frequencies.most_common(20):\n",
    "    print(f\"  '{word}': {count}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Word Frequencies After Stopword Removal:\")\n",
    "cleaned_frequencies = get_word_frequencies(product_df['Cleaned_Description_No_Stopwords'])\n",
    "# Print top 20 most common words\n",
    "for word, count in cleaned_frequencies.most_common(20):\n",
    "    print(f\"  '{word}': {count}\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Observation:\")\n",
    "print(\"You can observe that common words like 'is', 'a', 'the', 'with', 'for', 'it', 'has', 'was', 'will', 'you', 'should' etc., which are typically stopwords, have significantly reduced or disappeared from the 'Word Frequencies After Stopword Removal' list, indicating the effectiveness of the process.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
