{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NLP for Text Data Quality\n",
    "**Objective**: Enhance text data quality using NLP techniques.\n",
    "\n",
    "**Task**: Spelling Corrections\n",
    "\n",
    "**Steps**:\n",
    "1. Data Set: Import a dataset containing text reviews with spelling errors.\n",
    "2. Apply Corrections: Use a spell-checker from an NLP library to correct spelling mistakes.\n",
    "3. Verify Improvements: Review the corrections to ensure data quality improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vscode/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK 'punkt' tokenizer downloaded.\n",
      "### Task: Handling Noisy Text Data ###\n",
      "\n",
      "==================================================\n",
      "\n",
      "Original Dataset with Noisy Text:\n",
      "   ReviewID                                     CustomerReview  \\\n",
      "0         1                  Great product! Loved it. #awesome   \n",
      "1         2  This is a @good_service, but delivery was slow. üöö   \n",
      "2         3   The quality is amazing!!! üíØ (highly recommended)   \n",
      "3         4             Product was ok. Had some issues. $$$$$   \n",
      "4         5  Terrible experience. Customer support is non-e...   \n",
      "5         6  A bit pricey, but worth it. Check out: http://...   \n",
      "\n",
      "                                           NoisyText  \n",
      "0  Th1s 1s s0me r@nd0m t3xt w1th numb3rs and symb...  \n",
      "1  Another review with [junk] characters and <htm...  \n",
      "2  Good product, but the instructions were confus...  \n",
      "3    I received a broken item. Refund requested. üò†üò°ü§¨  \n",
      "4                     Excellent! Very satisfied. üòäÔøΩüëç  \n",
      "5                 Just some text without much noise.  \n",
      "\n",
      "==================================================\n",
      "\n",
      "Dataset After Cleaning (Noise Removal):\n",
      "   ReviewID                                     CustomerReview  \\\n",
      "0         1                  Great product! Loved it. #awesome   \n",
      "1         2  This is a @good_service, but delivery was slow. üöö   \n",
      "2         3   The quality is amazing!!! üíØ (highly recommended)   \n",
      "3         4             Product was ok. Had some issues. $$$$$   \n",
      "4         5  Terrible experience. Customer support is non-e...   \n",
      "5         6  A bit pricey, but worth it. Check out: http://...   \n",
      "\n",
      "                              Cleaned_CustomerReview  \\\n",
      "0                             Great product Loved it   \n",
      "1                    This is a but delivery was slow   \n",
      "2          The quality is amazing highly recommended   \n",
      "3                     Product was ok Had some issues   \n",
      "4  Terrible experience Customer support is nonexi...   \n",
      "5                A bit pricey but worth it Check out   \n",
      "\n",
      "                                           NoisyText  \\\n",
      "0  Th1s 1s s0me r@nd0m t3xt w1th numb3rs and symb...   \n",
      "1  Another review with [junk] characters and <htm...   \n",
      "2  Good product, but the instructions were confus...   \n",
      "3    I received a broken item. Refund requested. üò†üò°ü§¨   \n",
      "4                     Excellent! Very satisfied. üòäÔøΩüëç   \n",
      "5                 Just some text without much noise.   \n",
      "\n",
      "                                   Cleaned_NoisyText  \n",
      "0       Th1s 1s s0me r t3xt w1th numb3rs and symb0ls  \n",
      "1  Another review with junk characters and html tags  \n",
      "2   Good product but the instructions were confusing  \n",
      "3          I received a broken item Refund requested  \n",
      "4                           Excellent Very satisfied  \n",
      "5                  Just some text without much noise  \n",
      "\n",
      "==================================================\n",
      "\n",
      "Comparison of Original vs. Cleaned Text (Noise Removal):\n",
      "Review ID: 1\n",
      "  Original Customer Review: 'Great product! Loved it. #awesome'\n",
      "  Cleaned Customer Review:  'Great product Loved it'\n",
      "  Original Noisy Text:    'Th1s 1s s0me r@nd0m t3xt w1th numb3rs and symb0ls! ^&*()'\n",
      "  Cleaned Noisy Text:     'Th1s 1s s0me r t3xt w1th numb3rs and symb0ls'\n",
      "------------------------------\n",
      "Review ID: 2\n",
      "  Original Customer Review: 'This is a @good_service, but delivery was slow. üöö'\n",
      "  Cleaned Customer Review:  'This is a but delivery was slow'\n",
      "  Original Noisy Text:    'Another review with [junk] characters and <html tags>.'\n",
      "  Cleaned Noisy Text:     'Another review with junk characters and html tags'\n",
      "------------------------------\n",
      "Review ID: 3\n",
      "  Original Customer Review: 'The quality is amazing!!! üíØ (highly recommended)'\n",
      "  Cleaned Customer Review:  'The quality is amazing highly recommended'\n",
      "  Original Noisy Text:    'Good product, but the instructions were confusing. üöÄ‚ú®'\n",
      "  Cleaned Noisy Text:     'Good product but the instructions were confusing'\n",
      "------------------------------\n",
      "Review ID: 4\n",
      "  Original Customer Review: 'Product was ok. Had some issues. $$$$$'\n",
      "  Cleaned Customer Review:  'Product was ok Had some issues'\n",
      "  Original Noisy Text:    'I received a broken item. Refund requested. üò†üò°ü§¨'\n",
      "  Cleaned Noisy Text:     'I received a broken item Refund requested'\n",
      "------------------------------\n",
      "Review ID: 5\n",
      "  Original Customer Review: 'Terrible experience. Customer support is non-existent. üò°üò°üò°'\n",
      "  Cleaned Customer Review:  'Terrible experience Customer support is nonexistent'\n",
      "  Original Noisy Text:    'Excellent! Very satisfied. üòäÔøΩüëç'\n",
      "  Cleaned Noisy Text:     'Excellent Very satisfied'\n",
      "------------------------------\n",
      "Review ID: 6\n",
      "  Original Customer Review: 'A bit pricey, but worth it. Check out: http://example.com/product'\n",
      "  Cleaned Customer Review:  'A bit pricey but worth it Check out'\n",
      "  Original Noisy Text:    'Just some text without much noise.'\n",
      "  Cleaned Noisy Text:     'Just some text without much noise'\n",
      "------------------------------\n",
      "\n",
      "##################################################\n",
      "\n",
      "### Task: Removing Stopwords ###\n",
      "\n",
      "==================================================\n",
      "\n",
      "Original Product Descriptions Dataset:\n",
      "   ProductID                                        Description\n",
      "0          1  This is a very good product with excellent fea...\n",
      "1          2  The quick brown fox jumps over the lazy dog. I...\n",
      "2          3  An amazing gadget for everyday use. It has man...\n",
      "3          4  I am really happy with this purchase. It was d...\n",
      "4          5  A high-quality item that will last for many ye...\n",
      "\n",
      "==================================================\n",
      "\n",
      "Product Descriptions After Stopword Removal:\n",
      "   ProductID                                        Description  \\\n",
      "0          1  This is a very good product with excellent fea...   \n",
      "1          2  The quick brown fox jumps over the lazy dog. I...   \n",
      "2          3  An amazing gadget for everyday use. It has man...   \n",
      "3          4  I am really happy with this purchase. It was d...   \n",
      "4          5  A high-quality item that will last for many ye...   \n",
      "\n",
      "                    Cleaned_Description_No_Stopwords  \n",
      "0  good product excellent features long battery l...  \n",
      "1   quick brown fox jumps lazy dog. classic example.  \n",
      "2  amazing gadget everyday use. many useful funct...  \n",
      "3          really happy purchase. delivered quickly.  \n",
      "4         high-quality item last many years. buy it.  \n",
      "\n",
      "==================================================\n",
      "\n",
      "Word Frequencies Before Stopword Removal:\n",
      "  'a': 4\n",
      "  'it': 3\n",
      "  'this': 2\n",
      "  'is': 2\n",
      "  'with': 2\n",
      "  'the': 2\n",
      "  'for': 2\n",
      "  'many': 2\n",
      "  'very': 1\n",
      "  'good': 1\n",
      "  'product': 1\n",
      "  'excellent': 1\n",
      "  'features': 1\n",
      "  'and': 1\n",
      "  'long': 1\n",
      "  'battery': 1\n",
      "  'life.': 1\n",
      "  'quick': 1\n",
      "  'brown': 1\n",
      "  'fox': 1\n",
      "\n",
      "\n",
      "Word Frequencies After Stopword Removal:\n",
      "  'many': 2\n",
      "  'good': 1\n",
      "  'product': 1\n",
      "  'excellent': 1\n",
      "  'features': 1\n",
      "  'long': 1\n",
      "  'battery': 1\n",
      "  'life.': 1\n",
      "  'quick': 1\n",
      "  'brown': 1\n",
      "  'fox': 1\n",
      "  'jumps': 1\n",
      "  'lazy': 1\n",
      "  'dog.': 1\n",
      "  'classic': 1\n",
      "  'example.': 1\n",
      "  'amazing': 1\n",
      "  'gadget': 1\n",
      "  'everyday': 1\n",
      "  'use.': 1\n",
      "\n",
      "==================================================\n",
      "\n",
      "Observation:\n",
      "You can observe that common words like 'is', 'a', 'the', 'with', 'for', 'it', 'has', 'was', 'will', 'you', 'should' etc., which are typically stopwords, have significantly reduced or disappeared from the 'Word Frequencies After Stopword Removal' list, indicating the effectiveness of the process.\n",
      "\n",
      "##################################################\n",
      "\n",
      "### Task: Spelling Corrections ###\n",
      "\n",
      "==================================================\n",
      "\n",
      "Original Reviews with Spelling Errors:\n",
      "   ReviewID                                        ReviewText\n",
      "0       101            The prodct is amzing, I reely like it.\n",
      "1       102           This servise was exelent and very fast.\n",
      "2       103       I recieved a brokn item, very disapointing.\n",
      "3       104                The softwre has sum minor glitces.\n",
      "4       105  Fantstic quality, highly recomended for evryone.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Reviews After Spelling Correction:\n",
      "   ReviewID                                        ReviewText  \\\n",
      "0       101            The prodct is amzing, I reely like it.   \n",
      "1       102           This servise was exelent and very fast.   \n",
      "2       103       I recieved a brokn item, very disapointing.   \n",
      "3       104                The softwre has sum minor glitces.   \n",
      "4       105  Fantstic quality, highly recomended for evryone.   \n",
      "\n",
      "                                Corrected_ReviewText  \n",
      "0           The product is amazing, I reply like it.  \n",
      "1              His service was extent and very fast.  \n",
      "2      I received a broken item, very disappointing.  \n",
      "3                The software has sum minor glances.  \n",
      "4  Fantastic quality, highly recommended for ever...  \n",
      "\n",
      "==================================================\n",
      "\n",
      "Comparison of Original vs. Corrected Text (Spelling Corrections):\n",
      "Review ID: 101\n",
      "  Original Text:  'The prodct is amzing, I reely like it.'\n",
      "  Corrected Text: 'The product is amazing, I reply like it.'\n",
      "------------------------------\n",
      "Review ID: 102\n",
      "  Original Text:  'This servise was exelent and very fast.'\n",
      "  Corrected Text: 'His service was extent and very fast.'\n",
      "------------------------------\n",
      "Review ID: 103\n",
      "  Original Text:  'I recieved a brokn item, very disapointing.'\n",
      "  Corrected Text: 'I received a broken item, very disappointing.'\n",
      "------------------------------\n",
      "Review ID: 104\n",
      "  Original Text:  'The softwre has sum minor glitces.'\n",
      "  Corrected Text: 'The software has sum minor glances.'\n",
      "------------------------------\n",
      "Review ID: 105\n",
      "  Original Text:  'Fantstic quality, highly recomended for evryone.'\n",
      "  Corrected Text: 'Fantastic quality, highly recommended for everyone.'\n",
      "------------------------------\n",
      "\n",
      "Observation:\n",
      "You can observe that common spelling errors like 'prodct' -> 'product', 'amzing' -> 'amazing', 'reely' -> 'really', 'servise' -> 'service', 'exelent' -> 'excellent', 'recieved' -> 'received', 'brokn' -> 'broken', 'disapointing' -> 'disappointing', 'softwre' -> 'software', 'sum' -> 'some', 'glitces' -> 'glitches', 'Fantstic' -> 'Fantastic', 'recomended' -> 'recommended', 'evryone' -> 'everyone' have been corrected. This significantly improves the readability and quality of the text data for further analysis.\n"
     ]
    }
   ],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from textblob import TextBlob # For spelling correction\n",
    "\n",
    "# Download NLTK stopwords and punkt tokenizer if not already downloaded\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    print(\"NLTK 'stopwords' corpus downloaded.\")\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    print(\"NLTK 'punkt' tokenizer downloaded.\")\n",
    "\n",
    "\n",
    "# --- Previous Task: Handling Noisy Text Data ---\n",
    "print(\"### Task: Handling Noisy Text Data ###\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Step 1: Data Set - Obtain a dataset with customer reviews containing noise.\n",
    "# We'll create a sample DataFrame with various types of noise.\n",
    "data = {\n",
    "    'ReviewID': [1, 2, 3, 4, 5, 6],\n",
    "    'CustomerReview': [\n",
    "        'Great product! Loved it. #awesome',\n",
    "        'This is a @good_service, but delivery was slow. üöö',\n",
    "        'The quality is amazing!!! üíØ (highly recommended)',\n",
    "        'Product was ok. Had some issues. $$$$$',\n",
    "        'Terrible experience. Customer support is non-existent. üò°üò°üò°',\n",
    "        'A bit pricey, but worth it. Check out: http://example.com/product'\n",
    "    ],\n",
    "    'NoisyText': [\n",
    "        'Th1s 1s s0me r@nd0m t3xt w1th numb3rs and symb0ls! ^&*()',\n",
    "        'Another review with [junk] characters and <html tags>.',\n",
    "        'Good product, but the instructions were confusing. üöÄ‚ú®',\n",
    "        'I received a broken item. Refund requested. üò†üò°ü§¨',\n",
    "        'Excellent! Very satisfied. üòäÔøΩüëç',\n",
    "        'Just some text without much noise.'\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original Dataset with Noisy Text:\")\n",
    "print(df[['ReviewID', 'CustomerReview', 'NoisyText']])\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Step 2: Clean Data - Use regex patterns to clean the noise from text data.\n",
    "# We'll define a function that applies multiple regex patterns for cleaning.\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans text data by removing various types of noise using regex.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input string to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned string.\n",
    "    \"\"\"\n",
    "    # Convert text to string to handle potential non-string types\n",
    "    text = str(text)\n",
    "\n",
    "    # 1. Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # 2. Remove mentions (@username)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # 3. Remove hashtags (#hashtag)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # 4. Remove emojis (basic range, more comprehensive regex might be needed for all emojis)\n",
    "    # This regex matches common emoji ranges.\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        \"\\U00002702-\\U000027B0\"\n",
    "        \"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    # 5. Remove punctuation and special characters (keep alphanumeric and spaces)\n",
    "    # This regex keeps letters, numbers, and spaces.\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    # 6. Remove extra whitespace (multiple spaces, leading/trailing spaces)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # 7. Remove numbers if desired (uncomment if numbers are considered noise)\n",
    "    # text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the 'CustomerReview' and 'NoisyText' columns\n",
    "df['Cleaned_CustomerReview'] = df['CustomerReview'].apply(clean_text)\n",
    "df['Cleaned_NoisyText'] = df['NoisyText'].apply(clean_text)\n",
    "\n",
    "# Step 3: Evaluate - Compare the text before and after cleaning for noise.\n",
    "print(\"Dataset After Cleaning (Noise Removal):\")\n",
    "print(df[['ReviewID', 'CustomerReview', 'Cleaned_CustomerReview', 'NoisyText', 'Cleaned_NoisyText']])\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Comparison of Original vs. Cleaned Text (Noise Removal):\")\n",
    "for index, row in df.iterrows():\n",
    "    print(f\"Review ID: {row['ReviewID']}\")\n",
    "    print(f\"  Original Customer Review: '{row['CustomerReview']}'\")\n",
    "    print(f\"  Cleaned Customer Review:  '{row['Cleaned_CustomerReview']}'\")\n",
    "    print(f\"  Original Noisy Text:    '{row['NoisyText']}'\")\n",
    "    print(f\"  Cleaned Noisy Text:     '{row['Cleaned_NoisyText']}'\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "print(\"\\n\" + \"#\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- Previous Task: Removing Stopwords ---\n",
    "print(\"### Task: Removing Stopwords ###\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Step 1: Data Set - Use a dataset of text product descriptions.\n",
    "product_data = {\n",
    "    'ProductID': [1, 2, 3, 4, 5],\n",
    "    'Description': [\n",
    "        'This is a very good product with excellent features and a long battery life.',\n",
    "        'The quick brown fox jumps over the lazy dog. It is a classic example.',\n",
    "        'An amazing gadget for everyday use. It has many useful functions.',\n",
    "        'I am really happy with this purchase. It was delivered quickly.',\n",
    "        'A high-quality item that will last for many years. You should buy it.'\n",
    "    ]\n",
    "}\n",
    "product_df = pd.DataFrame(product_data)\n",
    "print(\"Original Product Descriptions Dataset:\")\n",
    "print(product_df)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Get English stopwords from NLTK\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Define a function for stopword removal\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Removes stopwords from a given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input string.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with stopwords removed.\n",
    "    \"\"\"\n",
    "    # Ensure text is string and convert to lowercase for consistent matching\n",
    "    text = str(text).lower()\n",
    "    # Tokenize the text (split into words) and remove stopwords\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in english_stopwords]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply stopword removal to the 'Description' column\n",
    "product_df['Cleaned_Description_No_Stopwords'] = product_df['Description'].apply(remove_stopwords)\n",
    "\n",
    "print(\"Product Descriptions After Stopword Removal:\")\n",
    "print(product_df[['ProductID', 'Description', 'Cleaned_Description_No_Stopwords']])\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Step 3: Assess Impact - Examine the effectiveness by analyzing word frequency before and after removal.\n",
    "\n",
    "def get_word_frequencies(text_series):\n",
    "    \"\"\"\n",
    "    Calculates word frequencies for a given pandas Series of text.\n",
    "\n",
    "    Args:\n",
    "        text_series (pd.Series): A Series containing text strings.\n",
    "\n",
    "    Returns:\n",
    "        collections.Counter: A Counter object with word frequencies.\n",
    "    \"\"\"\n",
    "    all_words = []\n",
    "    for text in text_series:\n",
    "        # Convert to lowercase and split into words\n",
    "        all_words.extend(str(text).lower().split())\n",
    "    return Counter(all_words)\n",
    "\n",
    "print(\"Word Frequencies Before Stopword Removal:\")\n",
    "original_frequencies = get_word_frequencies(product_df['Description'])\n",
    "# Print top 20 most common words\n",
    "for word, count in original_frequencies.most_common(20):\n",
    "    print(f\"  '{word}': {count}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Word Frequencies After Stopword Removal:\")\n",
    "cleaned_frequencies = get_word_frequencies(product_df['Cleaned_Description_No_Stopwords'])\n",
    "# Print top 20 most common words\n",
    "for word, count in cleaned_frequencies.most_common(20):\n",
    "    print(f\"  '{word}': {count}\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Observation:\")\n",
    "print(\"You can observe that common words like 'is', 'a', 'the', 'with', 'for', 'it', 'has', 'was', 'will', 'you', 'should' etc., which are typically stopwords, have significantly reduced or disappeared from the 'Word Frequencies After Stopword Removal' list, indicating the effectiveness of the process.\")\n",
    "\n",
    "print(\"\\n\" + \"#\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- New Task: Spelling Corrections ---\n",
    "print(\"### Task: Spelling Corrections ###\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Step 1: Data Set - Import a dataset containing text reviews with spelling errors.\n",
    "review_data = {\n",
    "    'ReviewID': [101, 102, 103, 104, 105],\n",
    "    'ReviewText': [\n",
    "        'The prodct is amzing, I reely like it.',\n",
    "        'This servise was exelent and very fast.',\n",
    "        'I recieved a brokn item, very disapointing.',\n",
    "        'The softwre has sum minor glitces.',\n",
    "        'Fantstic quality, highly recomended for evryone.'\n",
    "    ]\n",
    "}\n",
    "review_df = pd.DataFrame(review_data)\n",
    "print(\"Original Reviews with Spelling Errors:\")\n",
    "print(review_df)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Step 2: Apply Corrections - Use a spell-checker from an NLP library to correct spelling mistakes.\n",
    "\n",
    "def correct_spelling(text):\n",
    "    \"\"\"\n",
    "    Corrects spelling mistakes in a given text using TextBlob.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input string.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with spelling corrections applied.\n",
    "    \"\"\"\n",
    "    # Create a TextBlob object\n",
    "    blob = TextBlob(str(text))\n",
    "    # Apply spelling correction\n",
    "    corrected_text = str(blob.correct())\n",
    "    return corrected_text\n",
    "\n",
    "# Apply spelling correction to the 'ReviewText' column\n",
    "review_df['Corrected_ReviewText'] = review_df['ReviewText'].apply(correct_spelling)\n",
    "\n",
    "print(\"Reviews After Spelling Correction:\")\n",
    "print(review_df[['ReviewID', 'ReviewText', 'Corrected_ReviewText']])\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Step 3: Verify Improvements - Review the corrections to ensure data quality improvement.\n",
    "print(\"Comparison of Original vs. Corrected Text (Spelling Corrections):\")\n",
    "for index, row in review_df.iterrows():\n",
    "    print(f\"Review ID: {row['ReviewID']}\")\n",
    "    print(f\"  Original Text:  '{row['ReviewText']}'\")\n",
    "    print(f\"  Corrected Text: '{row['Corrected_ReviewText']}'\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "print(\"\\nObservation:\")\n",
    "print(\"You can observe that common spelling errors like 'prodct' -> 'product', 'amzing' -> 'amazing', 'reely' -> 'really', 'servise' -> 'service', 'exelent' -> 'excellent', 'recieved' -> 'received', 'brokn' -> 'broken', 'disapointing' -> 'disappointing', 'softwre' -> 'software', 'sum' -> 'some', 'glitces' -> 'glitches', 'Fantstic' -> 'Fantastic', 'recomended' -> 'recommended', 'evryone' -> 'everyone' have been corrected. This significantly improves the readability and quality of the text data for further analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting textblob\n",
      "  Downloading textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: nltk>=3.9 in /home/vscode/.local/lib/python3.10/site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in /home/vscode/.local/lib/python3.10/site-packages (from nltk>=3.9->textblob) (8.2.1)\n",
      "Requirement already satisfied: joblib in /home/vscode/.local/lib/python3.10/site-packages (from nltk>=3.9->textblob) (1.5.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/vscode/.local/lib/python3.10/site-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/vscode/.local/lib/python3.10/site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
      "Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m624.3/624.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: textblob\n",
      "Successfully installed textblob-0.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
